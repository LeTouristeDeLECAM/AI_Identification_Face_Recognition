{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'set_printoptions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\matth\\OneDrive - ECAM\\Documents\\Cours ECAM\\5MIN\\Q1 23-24\\Projet IA\\AI_Identification_Face_Recognition\\object_identification_face_recognition.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/matth/OneDrive%20-%20ECAM/Documents/Cours%20ECAM/5MIN/Q1%2023-24/Projet%20IA/AI_Identification_Face_Recognition/object_identification_face_recognition.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39multralytics\u001b[39;00m \u001b[39mimport\u001b[39;00m YOLO\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/OneDrive%20-%20ECAM/Documents/Cours%20ECAM/5MIN/Q1%2023-24/Projet%20IA/AI_Identification_Face_Recognition/object_identification_face_recognition.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mLibrary\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpackage_face_recognition\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mfr\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/OneDrive%20-%20ECAM/Documents/Cours%20ECAM/5MIN/Q1%2023-24/Projet%20IA/AI_Identification_Face_Recognition/object_identification_face_recognition.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive - ECAM\\Documents\\Cours ECAM\\5MIN\\Q1 23-24\\Projet IA\\AI_Identification_Face_Recognition\\venvFaceRecognition\\lib\\site-packages\\ultralytics\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Ultralytics YOLO ðŸš€, AGPL-3.0 license\u001b[39;00m\n\u001b[0;32m      3\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m8.0.207\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39multralytics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m RTDETR, SAM, YOLO\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39multralytics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfastsam\u001b[39;00m \u001b[39mimport\u001b[39;00m FastSAM\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39multralytics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnas\u001b[39;00m \u001b[39mimport\u001b[39;00m NAS\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive - ECAM\\Documents\\Cours ECAM\\5MIN\\Q1 23-24\\Projet IA\\AI_Identification_Face_Recognition\\venvFaceRecognition\\lib\\site-packages\\ultralytics\\models\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Ultralytics YOLO ðŸš€, AGPL-3.0 license\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mrtdetr\u001b[39;00m \u001b[39mimport\u001b[39;00m RTDETR\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msam\u001b[39;00m \u001b[39mimport\u001b[39;00m SAM\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39myolo\u001b[39;00m \u001b[39mimport\u001b[39;00m YOLO\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive - ECAM\\Documents\\Cours ECAM\\5MIN\\Q1 23-24\\Projet IA\\AI_Identification_Face_Recognition\\venvFaceRecognition\\lib\\site-packages\\ultralytics\\models\\rtdetr\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Ultralytics YOLO ðŸš€, AGPL-3.0 license\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m RTDETR\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpredict\u001b[39;00m \u001b[39mimport\u001b[39;00m RTDETRPredictor\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mval\u001b[39;00m \u001b[39mimport\u001b[39;00m RTDETRValidator\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive - ECAM\\Documents\\Cours ECAM\\5MIN\\Q1 23-24\\Projet IA\\AI_Identification_Face_Recognition\\venvFaceRecognition\\lib\\site-packages\\ultralytics\\models\\rtdetr\\model.py:10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Ultralytics YOLO ðŸš€, AGPL-3.0 license\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mInterface for Baidu's RT-DETR, a Vision Transformer-based real-time object detector. RT-DETR offers real-time\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mperformance and high accuracy, excelling in accelerated backends like CUDA with TensorRT. It features an efficient\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mFor more information on RT-DETR, visit: https://arxiv.org/pdf/2304.08069.pdf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39multralytics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n\u001b[0;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39multralytics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtasks\u001b[39;00m \u001b[39mimport\u001b[39;00m RTDETRDetectionModel\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpredict\u001b[39;00m \u001b[39mimport\u001b[39;00m RTDETRPredictor\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive - ECAM\\Documents\\Cours ECAM\\5MIN\\Q1 23-24\\Projet IA\\AI_Identification_Face_Recognition\\venvFaceRecognition\\lib\\site-packages\\ultralytics\\engine\\model.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Union\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39multralytics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcfg\u001b[39;00m \u001b[39mimport\u001b[39;00m TASK2DATA, get_cfg, get_save_dir\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39multralytics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhub\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m HUB_WEB_ROOT\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39multralytics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtasks\u001b[39;00m \u001b[39mimport\u001b[39;00m attempt_load_one_weight, guess_model_task, nn, yaml_model_load\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive - ECAM\\Documents\\Cours ECAM\\5MIN\\Q1 23-24\\Projet IA\\AI_Identification_Face_Recognition\\venvFaceRecognition\\lib\\site-packages\\ultralytics\\cfg\\__init__.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m SimpleNamespace\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Dict, List, Union\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39multralytics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (ASSETS, DEFAULT_CFG, DEFAULT_CFG_DICT, DEFAULT_CFG_PATH, LOGGER, RANK, ROOT, RUNS_DIR,\n\u001b[0;32m     11\u001b[0m                                SETTINGS, SETTINGS_YAML, TESTS_RUNNING, IterableSimpleNamespace, __version__, checks,\n\u001b[0;32m     12\u001b[0m                                colorstr, deprecation_warn, yaml_load, yaml_print)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Define valid tasks and modes\u001b[39;00m\n\u001b[0;32m     15\u001b[0m MODES \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpredict\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mexport\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtrack\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbenchmark\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\matth\\OneDrive - ECAM\\Documents\\Cours ECAM\\5MIN\\Q1 23-24\\Projet IA\\AI_Identification_Face_Recognition\\venvFaceRecognition\\lib\\site-packages\\ultralytics\\utils\\__init__.py:102\u001b[0m\n\u001b[0;32m     43\u001b[0m HELP_MSG \u001b[39m=\u001b[39m \\\n\u001b[0;32m     44\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39m    Usage examples for running YOLOv8:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39m    GitHub: https://github.com/ultralytics/ultralytics\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39m# Settings\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m torch\u001b[39m.\u001b[39;49mset_printoptions(linewidth\u001b[39m=\u001b[39m\u001b[39m320\u001b[39m, precision\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, profile\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    103\u001b[0m np\u001b[39m.\u001b[39mset_printoptions(linewidth\u001b[39m=\u001b[39m\u001b[39m320\u001b[39m, formatter\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mfloat_kind\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m{:11.5g}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat})  \u001b[39m# format short g, %precision=5\u001b[39;00m\n\u001b[0;32m    104\u001b[0m cv2\u001b[39m.\u001b[39msetNumThreads(\u001b[39m0\u001b[39m)  \u001b[39m# prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'set_printoptions'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import Library.package_face_recognition as fr\n",
    "import cv2\n",
    "import cv2.face\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Load the model YOLO\n",
    "model_YOLO = YOLO(\"Library/yolov8m.pt\")\n",
    "\n",
    "# Load the model LBPH\n",
    "\n",
    "model_lbph = cv2.face.LBPHFaceRecognizer_create()\n",
    "model_lbph.read(\"Library/lbph_trained_model.yml\")\n",
    "\n",
    "\n",
    "# Load the image\n",
    "\n",
    "image = cv2.imread(\"Images/face_recognition.jpg\")\n",
    "\n",
    "# Load the label dictionary\n",
    "label_dict = np.load('Library/label_dict.npy', allow_pickle=True).item()\n",
    "\n",
    "# Detect the objects\n",
    "results = model_YOLO.predict(image)\n",
    "\n",
    "result = results[0]\n",
    "box = result.boxes[0]\n",
    "\n",
    "\n",
    "# if object is a person then crop the image and detect the face\n",
    "for i in range(len(result.boxes)):\n",
    "    box = result.boxes[i]\n",
    "    cords = box.xyxy[0].tolist()\n",
    "    cords = [round(x) for x in cords]\n",
    "    class_id = result.names[box.cls[0].item()]\n",
    "    conf = round(box.conf[0].item(), 2)\n",
    "\n",
    "    \n",
    "    if class_id == 'person':\n",
    "        print (\"person find : \",cords)\n",
    "        cropped_image = fr.cut_image(image, cords)\n",
    "        cv2.imwrite(f\"image_copped1{cords}.jpg\", cropped_image)\n",
    "\n",
    "        cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # cv2.imshow('cropped_image', cropped_image)\n",
    "        # cv2.waitKey(0)\n",
    "        # cv2.destroyAllWindows()\n",
    "\n",
    "        try:\n",
    "            label, coordinates = fr.face_recognition(cropped_image, model_lbph)\n",
    "\n",
    "            # find the corresponding name for the integer label\n",
    "            for name, l in label_dict.items():\n",
    "                if l == label:\n",
    "                    print(name)\n",
    "                    break\n",
    "            cords = coordinates + cords\n",
    "                    \n",
    "                    \n",
    "        except:\n",
    "            name= class_id\n",
    "            print(\"No face detected 2\")\n",
    "\n",
    "        \n",
    "  \n",
    "        \n",
    "        print (cords)\n",
    "        image = fr.boundary_Name_Box(image, name, cords)\n",
    "        # cv2.imshow('image', image)\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "Image.fromarray(result.plot()[:,:,::-1]).save('image.jpg')\n",
    "\n",
    "cv2.imwrite(\"image_label.jpg\", image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8.1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
